{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio fingerprinting\n",
    "In this lab, you will practice with the core parts of the audio fingerprinting algorithm presented in http://www.ismir2002.ismir.net/proceedings/02-FP04-2.pdf.\n",
    "You will see how to fingerprint a song, and then will index a collection of about 100 songs.\n",
    "You will then have to implement a song retrieval algorithm employing these fingerprints, and match a number of 'unknown' song excerpts to the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to hand in\n",
    "We give you five 'unknown' excerpts. Your task is to employ the fingerprinting algorithm to identify to what files in your indexed dataset these excerpts correspond.\n",
    "\n",
    "As final deliverable to demonstrate your successful completion of this assignment, please submit a file named [studentNumberMember1_studentNumberMember2.pdf] through Brightspace with five lines of text, one for each excerpt.\n",
    "\n",
    "Each line should have the format \"<code>excerpt_file_name matched_file_name_in_your_indexed_dataset</code>\". The two filenames on each line are to be separated by a space.\n",
    "\n",
    "For example, a line could look like \"<code>excerpt-A.mp3 112233445.mp3</code>\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "As usual, we first import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "\n",
    "from cvtools import ipynb_show_matrix\n",
    "from datasets import CS4065_Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fetch and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to override the location where the dataset is stored (because you are not using the VM) \n",
    "# input the path as follows:\n",
    "# cs4065_dataset = CS4065_Dataset(os.path.join(...))\n",
    "cs4065_dataset = CS4065_Dataset()\n",
    "\n",
    "songretrieval_dataset = cs4065_dataset.get_songretrieval_subset()\n",
    "songretrieval_dataset_size = len(songretrieval_dataset)\n",
    "print('number of songs: {}'.format(songretrieval_dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at a random audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_index = np.random.randint(0, songretrieval_dataset_size)\n",
    "audio_file_keys = list(songretrieval_dataset.keys())\n",
    "audio_file_key = audio_file_keys[audio_file_index]  # File name.\n",
    "audio_file_path = songretrieval_dataset[audio_file_key]  # File path.\n",
    "print('random audio file: <{}>'.format(audio_file_key))\n",
    "\n",
    "signal, sample_rate = librosa.core.load(audio_file_path)\n",
    "assert len(np.shape(signal)) == 1, 'single channel expected'\n",
    "assert sample_rate == 22050, 'unexpected sample rate found'\n",
    "print('number of samples: {}'.format(len(signal)))\n",
    "print('sample rate: {}'.format(sample_rate))\n",
    "print('duration: {0:.1f} seconds'.format(float(len(signal)) / float(sample_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can render an audio player to listen to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.display(IPython.display.Audio(audio_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingerprint extraction\n",
    "Given a music signal, we want to summarize the auditory information that it contains. The summary has to be in the form of a collection of *fingerprints*. The computed fingerprints are meant to be used both at the *indexing* and *retrieval* step.\n",
    "\n",
    "When indexing, the fingerprints are stored in a data structure that is efficient for searching. This is because at the retrieval step, the fingerprints extracted from the excerpt to be recognized have to be matched against the indexed ones. A function that analyzes the found matches will finally determine whether the given excerpt corresponds to one of the indexed songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "Assuming that the input signal has a single channel (i.e., mono) and its sample rate is 22050 Hz, fingerpints are extracted using the following pipeline:\n",
    " 1. Apply a short-time Fourier Transform (STFT) (no overlap, 512 samples per frame) to retrieve the frequencies present in the signal\n",
    " 2. Compute the magnitude spectrum of the signal, representing the amount of energy for each of the frequencies\n",
    " 3. Applying the perceptual Bark scale (17 bands), compute the magnitude per band\n",
    " 4. Compute first-order derivatives in the frequency and time domain\n",
    " 5. Apply binarization to efficiently encode the derivatives.\n",
    "\n",
    "We will assist you in building the steps to this pipeline: see the blocks below. Upon setting up the pipeline, you will be asked to use it for indexing and retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-time Fourier Transform (STFT)\n",
    "We represent the input signal in the frequency domain over time. To this end, we divide the signal in non-overlapping frames of 512 samples each. For each frame, we compute the FFT. The output is a matrix of spectral coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stft(signal, frame_size = 512):\n",
    "    print('computing STFT')\n",
    "    signal = np.array(signal)\n",
    "    assert len(np.shape(signal)) == 1, 'single channel expected'\n",
    "    number_of_samples = len(signal)\n",
    "    number_of_full_frames = int(np.floor(float(number_of_samples) / float(frame_size)))\n",
    "    print(' - number of full frames: {}'.format(number_of_full_frames))\n",
    "\n",
    "    # Remove last frame (if not full).\n",
    "    print(' - original number of samples: {}'.format(number_of_samples))\n",
    "    number_of_samples = number_of_full_frames * frame_size\n",
    "    print(' - number of retained samples: {}'.format(number_of_samples))\n",
    "    signal = signal[:number_of_samples]\n",
    "    \n",
    "    # Split into frames (reshape to a matrix that has one row per frame).\n",
    "    signal_frames = np.reshape(signal, (number_of_full_frames, frame_size))\n",
    "\n",
    "    # Compute frame-wise FFT.\n",
    "    signal_stft = np.fft.fft(signal_frames).T\n",
    "    print(' - STFT matrix size: {} x {}'.format(*np.shape(signal_stft)))\n",
    "\n",
    "    return signal_stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now use this newly created function to compute the STFT\n",
    "signal_stft = compute_stft(signal)\n",
    "\n",
    "# Each row in signal_stft, that is a frequency bin, is associated to a frequency.\n",
    "# We can use numpy to get the frequencies of each bin.\n",
    "number_of_frequency_bins = np.shape(signal_stft)[0]\n",
    "frequencies = np.fft.fftfreq(number_of_frequency_bins, d=1.0 / float(sample_rate))\n",
    "\n",
    "# If we look at the first bin, we see that:\n",
    "# - the frequency is zero,\n",
    "# - all the values are real (the imaginary part is zero).\n",
    "print('first bin frequency: {0:.1f} hz'.format(frequencies[0]))\n",
    "print(signal_stft[0, :10])  # We only show the first 10 frames (but it holds for all the frames)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude spectrum\n",
    "For each frame, we have 512 frequency domain coefficients. The first half of each vector of spectral coefficients is the conjugate of the second half. We retain only the first half and compute the magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: compute the magnitude spectrum.\n",
    "\n",
    "def compute_magnitude_spectrum(signal_stft):\n",
    "    # Throw away redundant coefficients.\n",
    "    signal_stft = signal_stft[:257, :]\n",
    "    \n",
    "    # Flip matrix in order to have the highest frequency associated to\n",
    "    # the lowest row indexes (useful for visualization).\n",
    "    signal_stft = np.flipud(signal_stft)\n",
    "\n",
    "    # Compute the magnitude: module of each coefficient (note that we have complex coefficients).\n",
    "    return np.log(np.maximum(np.real(signal_stft * np.conjugate(signal_stft)), 1e-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the magnitude spectrum\n",
    "magnitude_spectrum = compute_magnitude_spectrum(signal_stft)\n",
    "print('magnitude spectrum matrix: {} x {}'.format(*np.shape(magnitude_spectrum)))\n",
    "ipynb_show_matrix(magnitude_spectrum, 'Magnitude spectrum', figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Band magnitudes\n",
    "We sum up the individual magnitudes per band using 17 given perceptual bands. The latter have been pre-determined using the Bark scale (see https://en.wikipedia.org/wiki/Bark_scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bark scale band boundaries.\n",
    "# Each band includes the indexes in the interval [index0, index1)\n",
    "# where index0 and index1 are adjacent values.\n",
    "BARK_SCALE_BANDS = [0, 2, 4, 7, 10, 13, 17, 21, 26, 32, 40, 50, 62, 79, 104, 136, 184, 257]\n",
    "\n",
    "def compute_band_magnitudes(magnitude_spectrum):\n",
    "    number_of_frames = np.shape(magnitude_spectrum)[1]\n",
    "    number_of_bands = len(BARK_SCALE_BANDS) - 1\n",
    "    band_magnitudes = np.zeros((number_of_bands, number_of_frames))\n",
    "    for band_index in range(number_of_bands):\n",
    "        index0 = BARK_SCALE_BANDS[band_index]\n",
    "        index1 = BARK_SCALE_BANDS[band_index + 1]\n",
    "        band_magnitudes[band_index, :] = np.sum(magnitude_spectrum[index0:index1, :], axis=0)\n",
    "\n",
    "    return band_magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now compute the band magnitudes\n",
    "band_magnitudes = compute_band_magnitudes(magnitude_spectrum)\n",
    "print('band magnitudes matrix: {} x {}'.format(*np.shape(band_magnitudes)))\n",
    "ipynb_show_matrix(band_magnitudes, 'Band magnitudes', figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency and time domain derivatives\n",
    "The absolute magnitude values computed above are not invariant to power level and additive noise. However, if we consider local differences along both time and frequency, we may expect to observe similar values when an excerpt has to be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_band_magnitudes_derivatives(band_magnitudes):    \n",
    "    # Differences between neighboring bands.\n",
    "    diff_frequency = band_magnitudes[1:, :] - band_magnitudes[:-1, :]\n",
    "\n",
    "    # Differences between neighboring frames.\n",
    "    diff_time = diff_frequency[:, 1:] - diff_frequency[:, :-1]\n",
    "\n",
    "    number_of_bands, number_of_frames = np.shape(band_magnitudes)\n",
    "    assert np.shape(diff_time) == (number_of_bands - 1, number_of_frames - 1)\n",
    "    return diff_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now take the derivatives over frequency and time\n",
    "band_magnitudes_diff = compute_band_magnitudes_derivatives(band_magnitudes)\n",
    "print('band magnitudes derivatives matrix: {} x {}'.format(*np.shape(band_magnitudes_diff)))\n",
    "ipynb_show_matrix(band_magnitudes_diff, 'Band magnitudes (derivatives)', figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization\n",
    "Finally, we further improve the robustness of the fingeprints (while reducing the amount of encoded data), by binarizing the matrix computed using <code>compute_band_magnitudes_derivatives()</code>. We simply set a zero when a value is negative, and one otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(band_magnitudes_diff):\n",
    "    binarized = np.ones(np.shape(band_magnitudes_diff), dtype=np.uint8)\n",
    "    binarized[band_magnitudes_diff < 0.0] = 0\n",
    "    return binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computed the binarization\n",
    "band_magnitudes_diff_binarized = binarize(band_magnitudes_diff)\n",
    "ipynb_show_matrix(band_magnitudes_diff_binarized, 'Band magnitudes (binarized derivatives)', figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question to you: index the collection of songs\n",
    "We are now ready to index the given songs. You have to iterate over the collection (see <code>songretrieval_dataset</code> above) and use the pipeline implemented above to produce a matrix of binarized band magnitude differences for each song. Store each matrix as numpy file using <code>np.save()</code> (see http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.save.html). You can store the files in the same directory where the MP3 files are (e.g., use the same file name and use .npy as extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put your indexing code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question to you: recognize an excerpt\n",
    "Assume that you are given 15 seconds long excerpts to recognize. As done at the indexing step, you have to extract the matrix of binarized magnitude differences computed by <code>binarize</code>. Then, you compare this matrix against *all* the matrices that you have stored as .npy files at the indexing step.\n",
    "\n",
    "The result of each comparison has to be a scalar value named *Bit Error Rate* (BER).\n",
    "The BER is simply computed as the number of differences between two binarized matrices (one belonging to the query excerpts and one to a candidate match) divided by the number of bits (i.e., the matrix size). When the query excerpt is shorter in duration than an indexed song, slice the indexed matrix accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put your code for recognizing an excerpt here.\n",
    "# It is advisable to also create a helper function for computing the Bit Error Rate between two binarized matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "We give you five 'unknown' excerpts. Your task is to employ the fingerprinting algorithm to identify to what files in your indexed dataset these excerpts correspond.\n",
    "\n",
    "Please submit a text file named [studentNumberMember1_studentNumberMember2.pdf] through Brightspace with five lines of text, one for each excerpt.\n",
    "\n",
    "Each line should have the format \"<code>excerpt_file_name matched_file_name_in_your_indexed_dataset</code>\". The two filenames on each line are to be separated by a space.\n",
    "\n",
    "For example, a line could look like \"<code>excerpt-A.mp3 112233445.mp3</code>\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the 'unknown' query excerpts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songretrieval_queries = cs4065_dataset.get_songretrieval_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to listen to them, you can render them all in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snippet in songretrieval_queries:\n",
    "    print('%s:'.format(snippet))\n",
    "    IPython.display.display(IPython.display.Audio(songretrieval_queries[snippet]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your code above to fingerprint and match these excerpts to the right songs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmsr",
   "language": "python",
   "name": "mmsr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
